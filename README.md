# ğŸš– NYC Taxi Data Ingestion with Apache Airflow

This project automates the **download and ingestion** of NYC Yellow Taxi data into a **PostgreSQL** database using **Apache Airflow**.  
---

## ğŸ§  Project Overview
The pipeline performs the following steps:

Download Dataset: Fetch monthly NYC Yellow Taxi trip data from the DataTalksClub GitHub releases.
Load to Postgres: Use Pandas and SQLAlchemy to load the dataset into a PostgreSQL table (yellow_taxi_data).
Orchestrate with Airflow: Manage task dependencies and scheduling with Airflow DAGs.

### ğŸ”¹ Tools Used
- **Apache Airflow** â€” workflow orchestration  
- **PostgreSQL** â€” data warehouse  
- **Docker & Docker Compose** â€” containerized setup  
- **Python, Pandas, SQLAlchemy** â€” data processing  
- **Redis + Celery** â€” distributed task queue for Airflow  

### ğŸ”¹ Workflow
The Airflow DAG (`data_ingestion_local`) performs:
1. **Download** â€” retrieves monthly NYC Yellow Taxi data (CSV.gz) from GitHub  
2. **Ingestion** â€” loads data into a PostgreSQL table (`yellow_taxi_data`)  

---

## âš™ï¸ Project Structure
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ data_ingestion_local.py # Airflow DAG
â”œâ”€â”€ data/ # Local data folder (mounted in containers)
â”œâ”€â”€ logs/ # Airflow logs
â”œâ”€â”€ plugins/ # Custom Airflow plugins (optional)
â”œâ”€â”€ docker-compose.yaml # Airflow multi-container setup
â”œâ”€â”€ .env # Environment variables (ignored in Git)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Prerequisites

- Docker and Docker Compose installed
- At least 4GB of free disk space
- Ports 8080 and 6543 available

### 2ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/Adeniceadebo/nyc-taxi-data-pipeline.git
cd nyc-taxi-data-pipeline/airflow
```

### 3ï¸âƒ£ Generate Fernet Key

Generate a Fernet key for Airflow encryption:

```bash
# Using Docker (recommended)
docker run --rm apache/airflow:2.9.1-python3.10 python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"

# Or using Python (if cryptography is installed)
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

### 4ï¸âƒ£ Configure Environment Variables

Copy the example environment file and add your Fernet key:

```bash
cp env.example .env
```

Edit `.env` and add your generated Fernet key:

```bash
AIRFLOW__CORE__FERNET_KEY=your_generated_fernet_key_here
```

### 5ï¸âƒ£ Set Airflow User ID (Optional)

On Linux, set the Airflow user ID to match your system user:

```bash
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

Add your Fernet key to the same file.

### 6ï¸âƒ£ Initialize and Start Services

Initialize Airflow database and create admin user:

```bash
docker-compose up airflow-init
```

Start all services:

```bash
docker-compose up -d
```

### 7ï¸âƒ£ Access Airflow Web UI

- Open your browser and navigate to: `http://localhost:8080`
- Login with:
  - Username: `airflow`
  - Password: `airflow`

### 8ï¸âƒ£ Access pgAdmin (Database Management)

- Open your browser and navigate to: `http://localhost:5050`
- Login with:
  - Email: `admin@admin.com`
  - Password: `admin`

**To connect to PostgreSQL from pgAdmin:**
1. Right-click on "Servers" â†’ "Register" â†’ "Server"
2. **General Tab:**
   - Name: `NYC Taxi Database` (or any name you prefer)
3. **Connection Tab:**
   - Host name/address: `postgres` (use the Docker service name)
   - Port: `5432`
   - Maintenance database: `airflow`
   - Username: `airflow`
   - Password: `airflow`
   - Check "Save password"
4. Click "Save"

**To access the ny_taxi database:**
- After connecting, expand "Databases" â†’ `ny_taxi` â†’ "Schemas" â†’ "public" â†’ "Tables"
- You'll see the `yellow_taxi_data` table after the pipeline runs

### 9ï¸âƒ£ Enable and Run the DAG

1. In the Airflow UI, find the `data_ingestion_local` DAG
2. Toggle it ON (unpause) using the switch on the left
3. The DAG will run monthly starting from 2021-01-01 to 2021-03-01
4. Monitor the progress in the Airflow UI

---

## ğŸ”§ Configuration

### Database Configuration

The pipeline uses a PostgreSQL database named `ny_taxi`. The database is automatically created during initialization.

- **Host**: `postgres` (internal Docker network) or `localhost` (external)
- **Port**: `5432` (internal), `6543` (external)
- **Database**: `ny_taxi` (for taxi data) or `airflow` (for Airflow metadata)
- **User**: `airflow`
- **Password**: `airflow`

### Accessing Services

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow Web UI** | http://localhost:8080 | Username: `airflow`, Password: `airflow` |
| **pgAdmin** | http://localhost:5050 | Email: `admin@admin.com`, Password: `admin` |
| **PostgreSQL** | localhost:6543 | User: `airflow`, Password: `airflow` |

### Customizing Database Settings

You can override database settings using Airflow Variables:

1. Go to Admin â†’ Variables in Airflow UI
2. Add variables:
   - `DB_USER`: Database username (default: `airflow`)
   - `DB_PASSWORD`: Database password (default: `airflow`)
   - `DB_HOST`: Database host (default: `postgres`)
   - `DB_PORT`: Database port (default: `5432`)
   - `DB_NAME`: Database name (default: `ny_taxi`)

---

## ğŸ“Š Data Pipeline Details

### DAG Schedule

- **Schedule**: Monthly (`@monthly`)
- **Start Date**: 2021-01-01
- **End Date**: 2021-03-01
- **Catchup**: Enabled (will backfill missing months)

### Tasks

1. **create_data_dir**: Ensures the data directory exists
2. **download_dataset**: Downloads monthly CSV.gz file from GitHub
3. **ingest_to_postgres**: Loads data into PostgreSQL in chunks (100k rows per chunk)

### Data Source

NYC Yellow Taxi trip data from [DataTalksClub GitHub releases](https://github.com/DataTalksClub/nyc-tlc-data/releases)

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Issue**: DAG fails with "database does not exist"
- **Solution**: Ensure `postgres-init` service completed successfully. Check logs: `docker-compose logs postgres-init`

**Issue**: Download task fails
- **Solution**: Check network connectivity and verify the URL is accessible

**Issue**: Ingestion task runs out of memory
- **Solution**: The pipeline uses chunked processing (100k rows). For very large files, reduce `chunk_size` in the DAG

**Issue**: Port already in use
- **Solution**: Change ports in `docker-compose.yaml` or stop conflicting services

### Viewing Logs

```bash
# All services
docker-compose logs

# Specific service
docker-compose logs airflow-scheduler
docker-compose logs airflow-worker

# Follow logs
docker-compose logs -f
```

### Stopping Services

```bash
# Stop services
docker-compose down

# Stop and remove volumes (âš ï¸ deletes data)
docker-compose down -v
```

---

## ğŸ“ Project Structure

```
airflow/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_ingestion_dag.py    # Main DAG definition
â”œâ”€â”€ data/                         # Downloaded data files (gitignored)
â”œâ”€â”€ logs/                         # Airflow logs (gitignored)
â”œâ”€â”€ plugins/                      # Custom Airflow plugins
â”œâ”€â”€ pgdata/                       # PostgreSQL data (gitignored)
â”œâ”€â”€ docker-compose.yaml           # Docker Compose configuration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ env.example                   # Environment variables template
â”œâ”€â”€ .env                          # Your environment variables (gitignored)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ”’ Security Notes

- **Default credentials**: Change default Airflow username/password in production
- **Database passwords**: Update PostgreSQL credentials in `docker-compose.yaml` for production
- **Fernet Key**: Always use a unique Fernet key in production
- **Environment variables**: Never commit `.env` file to version control

---

## ğŸ“ License

All rights reserved.
This project is for educational purposes as part of the Data Engineering Zoomcamp.
